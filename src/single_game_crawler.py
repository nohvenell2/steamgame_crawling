"""
Steam ê²Œì„ í¬ë¡¤ë§ì„ ìœ„í•œ ìµœì†Œí•œì˜ í¬ë¡¤ëŸ¬

ì´ ëª¨ë“ˆì€ Steam APIë¡œ ì œê³µë˜ì§€ ì•ŠëŠ” ì •ë³´ë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤:
- ì‚¬ìš©ìê°€ ë¶™ì¸ íƒœê·¸
- ë¦¬ë·° í†µê³„ (ê¸ì • ë¹„ìœ¨ ë“±)
- í˜„ì§€í™”ëœ ê°€ê²© ì •ë³´

ë°˜í™˜ êµ¬ì¡°:
- ì„±ê³µì‹œ: {'success': True, 'data': {...}}
- ì‹¤íŒ¨ì‹œ: {'success': False, 'error': 'error_type', 'message': 'error_message', 'app_id': app_id}

ê°€ëŠ¥í•œ ì—ëŸ¬ íƒ€ì…:
- 'age_verification_failed': ë‚˜ì´ ì¸ì¦ ì²˜ë¦¬ ì‹¤íŒ¨
- 'invalid_game_page': ìœ íš¨í•˜ì§€ ì•Šì€ ê²Œì„ í˜ì´ì§€
- 'rate_limit_exceeded': ìš”ì²­ ì œí•œ ì´ˆê³¼ (ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼)
- 'http_error': HTTP ì—ëŸ¬ (404, 500 ë“±)
- 'exception': ì˜ˆì™¸ ë°œìƒ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±)
- 'unknown': ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜

ì‚¬ìš© ì˜ˆì‹œ:
```python
import asyncio
from single_game_crawler_minimal import get_minimal_steam_info, setup_logger

# ë¡œê±° ì„¤ì • (ì„ íƒì‚¬í•­)
setup_logger("INFO")

# í”„ë¡œê·¸ë¨ ì‹¤í–‰
result = get_steam_game_info_sync(1091500)
print_game_info(result)
```
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
from typing import Dict, List, Optional, Any
import re
from datetime import datetime
import json
import csv
import os
from pathlib import Path
import logging

# ë¡œê±° ìœ í‹¸ë¦¬í‹° import
from utils.logger import setup_logger

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

class ComprehensiveGameCrawler:
    def __init__(self):
        self.base_url = "https://store.steampowered.com/app/"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

    def get_age_verification_cookies(self):
        """ë‚˜ì´ ì¸ì¦ì„ ìš°íšŒí•˜ê¸° ìœ„í•œ ì¿ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return {
            'birthtime': '1',
            'mature_content': '1',
            'lastagecheckage': '1-January-1970'
        }

    async def handle_age_check(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """ë‚˜ì´ ì¸ì¦ í˜ì´ì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            async with session.get(url, headers=self.headers) as response:
                if response.status == 200:
                    html = await response.text()
                    
                    if 'agecheck' in html or 'agegate' in html:
                        logger.info("ë‚˜ì´ ì¸ì¦ í˜ì´ì§€ ê°ì§€, ìš°íšŒ ì¤‘...")
                        form_data = {
                            'snr': '1_agecheck_agecheck__age-gate',
                            'ageDay': '1',
                            'ageMonth': 'January', 
                            'ageYear': '1990'
                        }
                        
                        async with session.post(url, data=form_data, headers=self.headers) as post_response:
                            if post_response.status == 200:
                                return await post_response.text()
                    
                    return html
        except Exception as e:
            logger.error(f"ë‚˜ì´ ì¸ì¦ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None

    def extract_basic_info(self, soup: BeautifulSoup, app_id: int) -> Dict[str, Any]:
        """ê¸°ë³¸ ê²Œì„ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        info = {}
        
        # ê²Œì„ ì œëª©
        title_selectors = [
            'div.apphub_AppName',
            'h1.pageheader',
            '.game_title h1',
            '#appHubAppName'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                info['title'] = element.get_text(strip=True)
                break
        
        if 'title' not in info:
            info['title'] = "Unknown"
        
        # ê²Œì„ ì§§ì€ ì„¤ëª…
        desc_selectors = [
            '.game_description_snippet',
            '.game_area_description .game_description_snippet'
        ]
        
        for selector in desc_selectors:
            element = soup.select_one(selector)
            if element:
                info['description'] = element.get_text(strip=True)
                break
        
        if 'description' not in info:
            info['description'] = ""
        
        return info

    def extract_detailed_description(self, soup: BeautifulSoup) -> str:
        """ê²Œì„ì˜ ìƒì„¸ ì„¤ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        detailed_desc = ""
        
        try:
            # ë°©ë²• 1: #game_area_descriptionì—ì„œ ì¶”ì¶œ
            game_area_desc = soup.select_one('#game_area_description')
            if game_area_desc:
                # "About This Game" ì œëª© ì œê±°í•˜ê³  ë‚´ìš©ë§Œ ì¶”ì¶œ
                desc_text = game_area_desc.get_text(strip=True)
                # "About This Game" ë¬¸êµ¬ ì œê±°
                if desc_text.startswith('About This Game'):
                    desc_text = desc_text[len('About This Game'):].strip()
                if desc_text:
                    detailed_desc = desc_text
            
            # ë°©ë²• 2: .game_area_description í´ë˜ìŠ¤ë¡œ ì°¾ê¸°
            if not detailed_desc:
                game_area_desc = soup.select_one('.game_area_description')
                if game_area_desc:
                    # ì§§ì€ ì„¤ëª… ìš”ì†ŒëŠ” ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    desc_copy = game_area_desc.__copy__()
                    # ì§§ì€ ì„¤ëª… ë¶€ë¶„ ì œê±°
                    short_desc_elem = desc_copy.select_one('.game_description_snippet')
                    if short_desc_elem:
                        short_desc_elem.decompose()
                    
                    desc_text = desc_copy.get_text(strip=True)
                    if desc_text.startswith('About This Game'):
                        desc_text = desc_text[len('About This Game'):].strip()
                    if desc_text and len(desc_text) > 200:  # ì¶©ë¶„íˆ ê¸´ í…ìŠ¤íŠ¸ë§Œ
                        detailed_desc = desc_text
            
            # ë°©ë²• 3: "About This Game" ì œëª© ë‹¤ìŒ ì»¨í…ì¸  ì°¾ê¸°
            if not detailed_desc:
                about_headers = soup.find_all(string=lambda text: text is not None and 'About This Game' in text)
                for header in about_headers:
                    parent = header.parent
                    if parent:
                        # ë‹¤ìŒ í˜•ì œ ìš”ì†Œë“¤ì—ì„œ ìƒì„¸ ì„¤ëª… ì°¾ê¸°
                        next_elem = parent.find_next_sibling()
                        while next_elem:
                            text = next_elem.get_text(strip=True)
                            if text and len(text) > 200:
                                detailed_desc = text
                                break
                            next_elem = next_elem.find_next_sibling()
                        
                        if detailed_desc:
                            break
            
            # ë°©ë²• 4: íŠ¹ì • ì»¨í…Œì´ë„ˆì—ì„œ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì°¾ê¸°
            if not detailed_desc:
                containers = soup.select('.tab_content, .game_page_autocollapse_ctn, .game_highlights')
                for container in containers:
                    text = container.get_text(strip=True)
                    if len(text) > 500 and len(text) > len(detailed_desc):
                        # ì§§ì€ ì„¤ëª…ê³¼ ë‹¤ë¥¸ ê²½ìš°ë§Œ
                        short_desc = soup.select_one('.game_description_snippet')
                        if short_desc:
                            short_text = short_desc.get_text(strip=True)
                            if text != short_text and short_text not in text[:200]:
                                detailed_desc = text
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            if detailed_desc:
                # ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
                detailed_desc = re.sub(r'\s+', ' ', detailed_desc).strip()
                # ë„ˆë¬´ ê¸´ ê²½ìš° ì œí•œ (5000ì)
                if len(detailed_desc) > 5000:
                    detailed_desc = detailed_desc[:5000] + "..."
                    
        except Exception as e:
            logger.error(f"ìƒì„¸ ì„¤ëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return detailed_desc

    def extract_tags(self, soup: BeautifulSoup) -> List[str]:
        """ê²Œì„ íƒœê·¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        tags = []
        tag_selectors = [
            'a.app_tag',
            '.popular_tags a',
            '.game_area_details_specs a'
        ]
        
        for selector in tag_selectors:
            tag_elements = soup.select(selector)
            if tag_elements:
                for tag in tag_elements:
                    tag_text = tag.get_text(strip=True)
                    if tag_text and tag_text not in tags:
                        tags.append(tag_text)
                break
        
        return tags

    def extract_genres(self, soup: BeautifulSoup) -> List[str]:
        """ê²Œì„ ì¥ë¥´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        genres = []
        
        try:
            # ë°©ë²• 1: ê²Œì„ ìƒì„¸ ì •ë³´ì—ì„œ "Genre:" ë¼ë²¨ ë‹¤ìŒì˜ ë§í¬ë“¤ ì°¾ê¸°
            genre_section = soup.select_one('#genresAndManufacturer')
            if genre_section:
                # "Genre:" í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë¶€ë¶„ ì°¾ê¸°
                genre_text = genre_section.get_text()
                if 'Genre:' in genre_text:
                    genre_links = genre_section.select('a[href*="/genre/"]')
                    for link in genre_links:
                        genre_name = link.get_text(strip=True)
                        if genre_name and genre_name not in genres:
                            genres.append(genre_name)
            
            # ë°©ë²• 2: ë” ì¼ë°˜ì ì¸ ë°©ë²•ìœ¼ë¡œ ì¥ë¥´ ë§í¬ ì°¾ê¸°
            if not genres:
                genre_links = soup.select('a[href*="/genre/"]')
                for link in genre_links:
                    # URLì—ì„œ ì¥ë¥´ëª… ì¶”ì¶œ
                    href = link.get('href')
                    if href and '/genre/' in href:
                        genre_name = link.get_text(strip=True)
                        # ê°œë°œì‚¬/í¼ë¸”ë¦¬ì…”ê°€ ì•„ë‹Œ ì‹¤ì œ ì¥ë¥´ì¸ì§€ í™•ì¸
                        if genre_name and len(genre_name) < 50 and genre_name not in genres:
                            # ì¼ë°˜ì ì¸ ì¥ë¥´ í‚¤ì›Œë“œ í•„í„°ë§
                            common_genres = [
                                'Action', 'Adventure', 'RPG', 'Strategy', 'Simulation', 
                                'Sports', 'Racing', 'Puzzle', 'Platformer', 'Shooter',
                                'Fighting', 'Horror', 'Survival', 'Arcade', 'Casual',
                                'Indie', 'MMO', 'Multiplayer', 'Co-op', 'VR', 'Free to Play',
                                'Early Access'
                            ]
                            if any(common in genre_name for common in common_genres):
                                genres.append(genre_name)
                            elif genre_name in common_genres:
                                genres.append(genre_name)
            
            # ë°©ë²• 3: details_blockì—ì„œ "Genre:" íŒ¨í„´ìœ¼ë¡œ ì°¾ê¸°
            if not genres:
                details_blocks = soup.select('.details_block')
                for block in details_blocks:
                    block_text = block.get_text()
                    if 'Genre:' in block_text:
                        # Genre: ë‹¤ìŒì˜ ë§í¬ë“¤ ì°¾ê¸°
                        genre_pattern = re.search(r'Genre:\s*(.+?)(?:\n|$)', block_text)
                        if genre_pattern:
                            genre_text = genre_pattern.group(1).strip()
                            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì¥ë¥´ë“¤ ë¶„ë¦¬
                            for genre in genre_text.split(','):
                                genre = genre.strip()
                                if genre and genre not in genres:
                                    genres.append(genre)
                        
                        # ë¸”ë¡ ë‚´ì˜ ì¥ë¥´ ë§í¬ë“¤ë„ ì°¾ê¸°
                        genre_links = block.select('a[href*="/genre/"]')
                        for link in genre_links:
                            genre_name = link.get_text(strip=True)
                            if genre_name and genre_name not in genres:
                                genres.append(genre_name)
                        break
            
            # ë°©ë²• 4: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ "Genre:" íŒ¨í„´ ì°¾ê¸°
            if not genres:
                all_text = soup.get_text()
                genre_matches = re.findall(r'Genre[s]?[:\s]+([^\n\r]+)', all_text, re.IGNORECASE)
                for match in genre_matches:
                    match = match.strip()
                    # ì‰¼í‘œë‚˜ ê¸°íƒ€ êµ¬ë¶„ìë¡œ ë¶„ë¦¬
                    for genre in re.split(r'[,;/]', match):
                        genre = genre.strip()
                        if genre and len(genre) < 50 and genre not in genres:
                            genres.append(genre)
            
        except Exception as e:
            logger.error(f"ì¥ë¥´ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        unique_genres = []
        for genre in genres:
            if genre and genre not in unique_genres:
                unique_genres.append(genre)
        
        return unique_genres

    def extract_price_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ê°€ê²© ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        price_info = {
            'current_price': None,
            'original_price': None,
            'discount_percent': None,
            'is_free': False
        }
        
        # ë¬´ë£Œ ê²Œì„ í™•ì¸
        free_elements = soup.select('.game_purchase_price')
        for elem in free_elements:
            text = elem.get_text(strip=True).lower()
            if 'free' in text or 'ë¬´ë£Œ' in text:
                price_info['is_free'] = True
                price_info['current_price'] = "Free"
                return price_info
        
        # í˜„ì¬ ê°€ê²©
        current_price_selectors = [
            '.game_purchase_price',
            '.discount_final_price'
        ]
        
        for selector in current_price_selectors:
            element = soup.select_one(selector)
            if element:
                price_text = element.get_text(strip=True)
                if price_text and price_text != '--':
                    price_info['current_price'] = price_text
                    break
        
        # ì›ë˜ ê°€ê²© (í• ì¸ ì‹œ)
        original_price_element = soup.select_one('.discount_original_price')
        if original_price_element:
            price_info['original_price'] = original_price_element.get_text(strip=True)
        
        # í• ì¸ìœ¨
        discount_element = soup.select_one('.discount_pct')
        if discount_element:
            discount_text = discount_element.get_text(strip=True)
            # "-50%" í˜•íƒœì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
            discount_match = re.search(r'-(\d+)%', discount_text)
            if discount_match:
                price_info['discount_percent'] = int(discount_match.group(1))
        
        return price_info

    def extract_developer_publisher(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ê°œë°œì‚¬/í¼ë¸”ë¦¬ì…” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        dev_pub_info: Dict[str, Any] = {
            'developer': None,
            'publisher': None
        }
        
        # ê°œë°œì‚¬/í¼ë¸”ë¦¬ì…” ì •ë³´ ì¶”ì¶œ - ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„
        try:
            # ë°©ë²• 1: ê²Œì„ ìƒì„¸ ì •ë³´ì—ì„œ ì°¾ê¸°
            details_specs = soup.select('.game_area_details_specs .summary')
            
            for i, spec in enumerate(details_specs):
                text = spec.get_text(strip=True).lower()
                
                # ë‹¤ìŒ ìš”ì†Œê°€ ìˆëŠ”ì§€ í™•ì¸
                if i + 1 < len(details_specs):
                    next_spec = details_specs[i + 1]
                    value = next_spec.get_text(strip=True)
                    
                    if 'developer' in text:
                        dev_pub_info['developer'] = value
                    elif 'publisher' in text:
                        dev_pub_info['publisher'] = value
            
            # ë°©ë²• 2: dev_row í´ë˜ìŠ¤ë¡œ ì°¾ê¸°
            if not dev_pub_info['developer'] or not dev_pub_info['publisher']:
                dev_rows = soup.select('.dev_row')
                for row in dev_rows:
                    summary = row.select_one('.summary')
                    if summary:
                        summary_text = summary.get_text(strip=True).lower()
                        if 'developer' in summary_text:
                            dev_link = row.select_one('a')
                            if dev_link:
                                dev_pub_info['developer'] = dev_link.get_text(strip=True)
                        elif 'publisher' in summary_text:
                            pub_link = row.select_one('a')
                            if pub_link:
                                dev_pub_info['publisher'] = pub_link.get_text(strip=True)
            
            # ë°©ë²• 3: ë§í¬ href ì†ì„±ìœ¼ë¡œ ì°¾ê¸°
            if not dev_pub_info['developer']:
                dev_elements = soup.select('a[href*="developer"]')
                if dev_elements:
                    dev_pub_info['developer'] = dev_elements[0].get_text(strip=True)
            
            if not dev_pub_info['publisher']:
                pub_elements = soup.select('a[href*="publisher"]')
                if pub_elements:
                    dev_pub_info['publisher'] = pub_elements[0].get_text(strip=True)
            
            # ë°©ë²• 4: ê²Œì„ ì •ë³´ ë¸”ë¡ì—ì„œ ì§ì ‘ ì°¾ê¸°
            if not dev_pub_info['developer'] or not dev_pub_info['publisher']:
                # ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ "Developer:" ë˜ëŠ” "Publisher:" íŒ¨í„´ ì°¾ê¸°
                all_text = soup.get_text()
                
                # Developer íŒ¨í„´ ì°¾ê¸°
                dev_match = re.search(r'Developer[:\s]+([^\n\r]+)', all_text, re.IGNORECASE)
                if dev_match and not dev_pub_info['developer']:
                    dev_pub_info['developer'] = dev_match.group(1).strip()
                
                # Publisher íŒ¨í„´ ì°¾ê¸°
                pub_match = re.search(r'Publisher[:\s]+([^\n\r]+)', all_text, re.IGNORECASE)
                if pub_match and not dev_pub_info['publisher']:
                    dev_pub_info['publisher'] = pub_match.group(1).strip()
            
            # ë°©ë²• 5: íŠ¹ì • í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ì°¾ê¸°
            if not dev_pub_info['developer'] or not dev_pub_info['publisher']:
                # ë‹¤ë¥¸ ê°€ëŠ¥í•œ ì„ íƒìë“¤
                possible_selectors = [
                    '.glance_ctn .summary',
                    '.details_block .summary',
                    '.game_area_details .summary'
                ]
                
                for selector in possible_selectors:
                    elements = soup.select(selector)
                    for i, elem in enumerate(elements):
                        text = elem.get_text(strip=True).lower()
                        if 'developer' in text and i + 1 < len(elements):
                            if not dev_pub_info['developer']:
                                dev_pub_info['developer'] = elements[i + 1].get_text(strip=True)
                        elif 'publisher' in text and i + 1 < len(elements):
                            if not dev_pub_info['publisher']:
                                dev_pub_info['publisher'] = elements[i + 1].get_text(strip=True)
                    
                    if dev_pub_info['developer'] and dev_pub_info['publisher']:
                        break
                        
        except Exception as e:
            logger.error(f"ê°œë°œì‚¬/í¼ë¸”ë¦¬ì…” ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return dev_pub_info

    def extract_release_date(self, soup: BeautifulSoup) -> Optional[str]:
        """ì¶œì‹œì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        release_selectors = [
            '.release_date .date',
            '.game_area_release_date .date'
        ]
        
        for selector in release_selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text(strip=True)
        
        return None

    def extract_review_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ë¦¬ë·°/í‰ì  ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        review_info: Dict[str, Any] = {
            'recent_reviews': None,
            'all_reviews': None,
            'recent_review_count': None,
            'total_review_count': None,
            'recent_positive_percent': None,
            'total_positive_percent': None
        }
        
        # ë¦¬ë·° ìš”ì•½ ì •ë³´
        review_summaries = soup.select('.game_review_summary')
        if len(review_summaries) >= 2:
            review_info['recent_reviews'] = review_summaries[0].get_text(strip=True)
            review_info['all_reviews'] = review_summaries[1].get_text(strip=True)
        elif len(review_summaries) == 1:
            review_info['all_reviews'] = review_summaries[0].get_text(strip=True)
        
        # ìƒì„¸ ë¦¬ë·° í†µê³„ - ë” ì •í™•í•œ ì¶”ì¶œ
        try:
            # ë¦¬ë·° í†µê³„ ì •ë³´ê°€ ìˆëŠ” ì„¹ì…˜ ì°¾ê¸°
            review_sections = soup.select('.user_reviews_summary_row')
            
            for section in review_sections:
                section_text = section.get_text()
                
                # ìµœê·¼ ë¦¬ë·° ì„¹ì…˜
                if 'Recent Reviews' in section_text or 'ìµœê·¼ ë¦¬ë·°' in section_text:
                    # ê´„í˜¸ ì•ˆì˜ ìˆ«ì ì¶”ì¶œ (ë¦¬ë·° ê°œìˆ˜)
                    count_match = re.search(r'\((\d{1,3}(?:,\d{3})*)\)', section_text)
                    if count_match:
                        review_info['recent_review_count'] = count_match.group(1)
                    
                    # í¼ì„¼íŠ¸ ì¶”ì¶œ - ë” ì •í™•í•œ íŒ¨í„´
                    percent_match = re.search(r'(\d+)%\s+of\s+the\s+\d+', section_text)
                    if percent_match:
                        review_info['recent_positive_percent'] = int(percent_match.group(1))
                
                # ì „ì²´ ë¦¬ë·° ì„¹ì…˜
                elif 'All Reviews' in section_text or 'ëª¨ë“  ë¦¬ë·°' in section_text:
                    # ê´„í˜¸ ì•ˆì˜ ìˆ«ì ì¶”ì¶œ (ë¦¬ë·° ê°œìˆ˜)
                    count_match = re.search(r'\((\d{1,3}(?:,\d{3})*)\)', section_text)
                    if count_match:
                        review_info['total_review_count'] = count_match.group(1)
                    
                    # í¼ì„¼íŠ¸ ì¶”ì¶œ - ë” ì •í™•í•œ íŒ¨í„´
                    percent_match = re.search(r'(\d+)%\s+of\s+the\s+\d+', section_text)
                    if percent_match:
                        review_info['total_positive_percent'] = int(percent_match.group(1))
            
            # ì¶”ê°€ ì‹œë„: ë‹¤ë¥¸ ì„ íƒìë¡œ ë¦¬ë·° í†µê³„ ì°¾ê¸°
            if not review_info['total_review_count']:
                # ë¦¬ë·° í†µê³„ë¥¼ ë‹´ê³  ìˆëŠ” ë‹¤ë¥¸ ìš”ì†Œë“¤ ì‹œë„
                review_stats = soup.select('.responsive_reviewdesc')
                for stat in review_stats:
                    stat_text = stat.get_text()
                    
                    # "94% of the 123,456 user reviews" íŒ¨í„´ ì°¾ê¸°
                    match = re.search(r'(\d+)%\s+of\s+the\s+([\d,]+)\s+user\s+reviews', stat_text)
                    if match:
                        review_info['total_positive_percent'] = int(match.group(1))
                        review_info['total_review_count'] = match.group(2)
                        break
                        
        except Exception as e:
            logger.error(f"ë¦¬ë·° ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return review_info

    def extract_header_images(self, soup: BeautifulSoup, app_id: int) -> List[str]:
        """í—¤ë” ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        header_images = []
        
        # HTMLì—ì„œ í—¤ë” ì´ë¯¸ì§€ ì°¾ê¸°
        header_selectors = [
            '.game_header_image_full',
            '.game_header_image img',
            '.page_header_image img'
        ]
        
        for selector in header_selectors:
            elements = soup.select(selector)
            for elem in elements:
                src = elem.get('src', '')
                if src and src not in header_images:
                    header_images.append(src)
        
        # Steamì˜ í‘œì¤€ í—¤ë” ì´ë¯¸ì§€ URL íŒ¨í„´ ì¶”ê°€
        standard_header_url = f"https://shared.fastly.steamstatic.com/store_item_assets/steam/apps/{app_id}/header.jpg"
        if standard_header_url not in header_images:
            header_images.append(standard_header_url)
        
        return header_images

    def extract_system_requirements(self, soup: BeautifulSoup) -> Dict[str, str]:
        """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        sys_req = {
            'minimum': '',
            'recommended': ''
        }
        
        sys_req_sections = soup.select('.game_area_sys_req')
        for section in sys_req_sections:
            text = section.get_text()
            if 'Minimum' in text or 'ìµœì†Œ' in text:
                sys_req['minimum'] = text.strip()
            elif 'Recommended' in text or 'ê¶Œì¥' in text:
                sys_req['recommended'] = text.strip()
        
        return sys_req

    async def get_comprehensive_game_info(self, app_id: int, max_retries: int = 7) -> Dict[str, Any]:
        """
        ê²Œì„ IDë¥¼ ì…ë ¥ë°›ì•„ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Returns:
            Dict[str, Any]: 
                ì„±ê³µì‹œ: {'success': True, 'data': {...}}
                ì‹¤íŒ¨ì‹œ: {'success': False, 'error': 'error_type', 'message': 'error_message', 'app_id': app_id}
        """
        url = f"{self.base_url}{app_id}"
        logger.info(f"ì¢…í•© ì •ë³´ ì¶”ì¶œ ì¤‘: {url}")
        
        for attempt in range(max_retries + 1):
            try:
                jar = aiohttp.CookieJar(unsafe=True)
                async with aiohttp.ClientSession(cookie_jar=jar) as session:
                    cookies = self.get_age_verification_cookies()
                    
                    async with session.get(url, headers=self.headers, cookies=cookies) as response:
                        # ì„±ê³µì ì¸ ì‘ë‹µ
                        if response.status == 200:
                            html = await response.text()
                            
                            # ë‚˜ì´ ì¸ì¦ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ ê²½ìš° ì²˜ë¦¬
                            if 'agecheck' in response.url.path or 'agegate' in html.lower():
                                html = await self.handle_age_check(session, str(response.url))
                                if not html:
                                    return {
                                        'success': False,
                                        'error': 'age_verification_failed',
                                        'message': 'ë‚˜ì´ ì¸ì¦ ì²˜ë¦¬ ì‹¤íŒ¨',
                                        'app_id': app_id
                                    }
                            
                            soup = BeautifulSoup(html, 'html.parser')
                            
                            # ëª¨ë“  ì •ë³´ ì¶”ì¶œ
                            game_info = {
                                'app_id': app_id,
                                'crawled_at': datetime.now().isoformat(),
                                **self.extract_basic_info(soup, app_id),
                                'detailed_description': self.extract_detailed_description(soup),
                                'tags': self.extract_tags(soup),
                                'genres': self.extract_genres(soup),
                                'price_info': self.extract_price_info(soup),
                                'developer_publisher': self.extract_developer_publisher(soup),
                                'release_date': self.extract_release_date(soup),
                                'review_info': self.extract_review_info(soup),
                                'header_images': self.extract_header_images(soup, app_id),
                                'system_requirements': self.extract_system_requirements(soup)
                            }
                            if not game_info.get('title') or game_info.get('title') == 'Unknown' :
                                logger.error(f"ê²Œì„ ID {app_id}: ê²Œì„ ì •ë³´ ì¶”ì¶œ ë¶ˆê°€ëŠ¥")
                                return {
                                    'success': False,
                                    'error': 'invalid_game_page',
                                    'message': 'ê²Œì„ ì •ë³´ ì¶”ì¶œ ë¶ˆê°€ëŠ¥ (ì œëª© ì—†ìŒ)',
                                    'app_id': app_id
                                }
                            
                            logger.info(f"ê²Œì„ ID {app_id}: ì¢…í•© ì •ë³´ ì¶”ì¶œ ì„±ê³µ - {game_info['title']}")
                            return {
                                'success': True,
                                'data': game_info
                            }
                        
                        # ìš”ì²­ ì œí•œ ê´€ë ¨ ìƒíƒœ ì½”ë“œ
                        elif response.status in [429, 503, 502, 504]:
                            if attempt < max_retries:
                                # ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ë”œë ˆì´ (2ì´ˆ, 4ì´ˆ, 8ì´ˆ, 16ì´ˆ, 32ì´ˆ, 64ì´ˆ, 128ì´ˆ)
                                delay = 2 * (2 ** attempt)
                                logger.warning(f"ê²Œì„ ID {app_id}: HTTP {response.status} - {attempt + 1}íšŒ ì‹¤íŒ¨, {delay}ì´ˆ í›„ ì¬ì‹œë„...")
                                await asyncio.sleep(delay)
                                continue
                            else:
                                logger.error(f"ê²Œì„ ID {app_id}: HTTP {response.status} - ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                                return {
                                    'success': False,
                                    'error': 'rate_limit_exceeded',
                                    'message': f'HTTP {response.status} - ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ({max_retries}) ì´ˆê³¼',
                                    'app_id': app_id,
                                    'http_status': response.status
                                }
                        
                        # ê¸°íƒ€ HTTP ì—ëŸ¬
                        else:
                            logger.error(f"ê²Œì„ ID {app_id}: HTTP {response.status} ì˜¤ë¥˜")
                            return {
                                'success': False,
                                'error': 'http_error',
                                'message': f'HTTP {response.status} ì˜¤ë¥˜',
                                'app_id': app_id,
                                'http_status': response.status
                            }
                            
            except Exception as e:
                if attempt < max_retries and "HTTP" in str(e) and any(code in str(e) for code in ["429", "503", "502", "504"]):
                    # ì¬ì‹œë„ ê°€ëŠ¥í•œ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜
                    delay = 2 * (2 ** attempt)
                    logger.warning(f"ê²Œì„ ID {app_id}: HTTP {response.status} - {attempt + 1}íšŒ ì‹¤íŒ¨, {delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(delay)
                    continue
                elif attempt == max_retries:
                    logger.error(f"ê²Œì„ ID {app_id}: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ - {str(e)}")
                    return {
                        'success': False,
                        'error': 'exception',
                        'message': f'ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ({max_retries}) ì´ˆê³¼ - {str(e)}',
                        'app_id': app_id,
                        'exception_type': type(e).__name__
                    }
                else:
                    logger.error(f"ê²Œì„ ID {app_id}: ì¢…í•© ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ - {str(e)}")
                    return {
                        'success': False,
                        'error': 'exception',
                        'message': f'ì¢…í•© ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ - {str(e)}',
                        'app_id': app_id,
                        'exception_type': type(e).__name__
                    }
        
        # ì´ë¡ ì ìœ¼ë¡œ ë„ë‹¬í•˜ì§€ ì•ŠëŠ” ì½”ë“œì´ì§€ë§Œ ì•ˆì „ì„ ìœ„í•´
        return {
            'success': False,
            'error': 'unknown',
            'message': 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜',
            'app_id': app_id
        }


# í¸ì˜ í•¨ìˆ˜ë“¤
async def get_steam_game_info_crawler(app_id: int, max_retries: int = 7) -> Dict[str, Any]:
    """
    Steam ê²Œì„ IDë¥¼ ì…ë ¥ë°›ì•„ ëª¨ë“  ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜
    
    Args:
        app_id (int): Steam ê²Œì„ ID
        max_retries (int): ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 7 - ì´ ëŒ€ê¸°ì‹œê°„ ì•½ 5ë¶„)
        
    Returns:
        Dict[str, Any]: 
            ì„±ê³µì‹œ: {'success': True, 'data': {...}}
            ì‹¤íŒ¨ì‹œ: {'success': False, 'error': 'error_type', 'message': 'error_message', 'app_id': app_id}
        
    Example:
        result = await get_steam_game_info(1091500)  # Cyberpunk 2077
        if result['success']:
            info = result['data']
            print(f"ì œëª©: {info['title']}")
            print(f"ê°€ê²©: {info['price_info']['current_price']}")
            print(f"íƒœê·¸: {info['tags']}")
        else:
            print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {result['error']} - {result['message']}")
    """
    crawler = ComprehensiveGameCrawler()
    return await crawler.get_comprehensive_game_info(app_id, max_retries)


def get_steam_game_info_crawler_sync(app_id: int, max_retries: int = 7) -> Dict[str, Any]:
    """
    Steam ê²Œì„ IDë¥¼ ì…ë ¥ë°›ì•„ ëª¨ë“  ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” ë™ê¸° í•¨ìˆ˜
    
    Args:
        app_id (int): Steam ê²Œì„ ID
        max_retries (int): ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 7 - ì´ ëŒ€ê¸°ì‹œê°„ ì•½ 5ë¶„)
        
    Returns:
        Dict[str, Any]: 
            ì„±ê³µì‹œ: {'success': True, 'data': {...}}
            ì‹¤íŒ¨ì‹œ: {'success': False, 'error': 'error_type', 'message': 'error_message', 'app_id': app_id}
        
    Example:
        result = get_steam_game_info_sync(1091500)  # Cyberpunk 2077
        if result['success']:
            info = result['data']
            print(f"ì œëª©: {info['title']}")
            print(f"ê°€ê²©: {info['price_info']['current_price']}")
            print(f"íƒœê·¸: {info['tags']}")
        else:
            print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {result['error']} - {result['message']}")
    """
    return asyncio.run(get_steam_game_info_crawler(app_id, max_retries))


def save_game_info_json(result: Dict[str, Any], filename: Optional[str] = None) -> str:
    """ê²Œì„ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not result:
        raise ValueError("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    if not result.get('success', False):
        raise ValueError("ì‹¤íŒ¨í•œ ê²°ê³¼ëŠ” ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    game_info = result['data']
    
    # íŒŒì¼ëª… ìƒì„±
    if not filename:
        app_id = game_info.get('app_id', 'unknown')
        title = game_info.get('title', 'unknown').replace('/', '_').replace('\\', '_')
        # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
        safe_title = re.sub(r'[<>:"|?*]', '', title)[:50]
        filename = f"game_{app_id}_{safe_title}.json"
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    save_dir = Path("data/game_info")
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # íŒŒì¼ ê²½ë¡œ
    file_path = save_dir / filename
    
    # JSONìœ¼ë¡œ ì €ì¥
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(game_info, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ê²Œì„ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {file_path}")
    return str(file_path)


def save_multiple_games_csv(results: List[Dict[str, Any]], filename: Optional[str] = None) -> str:
    """ì—¬ëŸ¬ ê²Œì„ ì •ë³´ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not results:
        raise ValueError("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
    successful_results = [result for result in results if result.get('success', False)]
    
    if not successful_results:
        raise ValueError("ì„±ê³µí•œ ê²°ê³¼ê°€ ì—†ì–´ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    games_info = [result['data'] for result in successful_results]
    
    # íŒŒì¼ëª… ìƒì„±
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"games_info_{timestamp}.csv"
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    save_dir = Path("data/game_info")
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # íŒŒì¼ ê²½ë¡œ
    file_path = save_dir / filename
    
    # CSV í—¤ë” ì •ì˜
    headers = [
        'app_id', 'title', 'description', 'detailed_description', 'current_price', 'original_price', 
        'discount_percent', 'is_free', 'developer', 'publisher', 'release_date',
        'all_reviews', 'total_review_count', 'total_positive_percent',
        'tags', 'genres', 'header_image', 'crawled_at'
    ]
    
    # CSVë¡œ ì €ì¥
    with open(file_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        
        for game_info in games_info:
            # ë°ì´í„° í‰íƒ„í™”
            row = {
                'app_id': game_info.get('app_id', ''),
                'title': game_info.get('title', ''),
                'description': game_info.get('description', '')[:200] + '...' if len(game_info.get('description', '')) > 200 else game_info.get('description', ''),
                'detailed_description': game_info.get('detailed_description', '')[:500] + '...' if len(game_info.get('detailed_description', '')) > 500 else game_info.get('detailed_description', ''),
                'current_price': game_info.get('price_info', {}).get('current_price', ''),
                'original_price': game_info.get('price_info', {}).get('original_price', ''),
                'discount_percent': game_info.get('price_info', {}).get('discount_percent', ''),
                'is_free': game_info.get('price_info', {}).get('is_free', False),
                'developer': game_info.get('developer_publisher', {}).get('developer', ''),
                'publisher': game_info.get('developer_publisher', {}).get('publisher', ''),
                'release_date': game_info.get('release_date', ''),
                'all_reviews': game_info.get('review_info', {}).get('all_reviews', ''),
                'total_review_count': game_info.get('review_info', {}).get('total_review_count', ''),
                'total_positive_percent': game_info.get('review_info', {}).get('total_positive_percent', ''),
                'tags': ', '.join(game_info.get('tags', [])),
                'genres': ', '.join(game_info.get('genres', [])),
                'header_image': game_info.get('header_images', [''])[0] if game_info.get('header_images') else '',
                'crawled_at': game_info.get('crawled_at', '')
            }
            writer.writerow(row)
    
    logger.info(f"{len(games_info)}ê°œ ê²Œì„ ì •ë³´ê°€ CSVë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {file_path}")
    return str(file_path)


def load_game_info_json(file_path: str) -> Dict[str, Any]:
    """JSON íŒŒì¼ì—ì„œ ê²Œì„ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def print_game_info(result: Dict[str, Any]):
    """ê²Œì„ ì •ë³´ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    if not result:
        print("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì‹¤íŒ¨í•œ ê²½ìš°
    if not result.get('success', False):
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨ (ID: {result.get('app_id', 'Unknown')})")
        print(f"   ì˜¤ë¥˜ íƒ€ì…: {result.get('error', 'unknown')}")
        print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        if 'http_status' in result:
            print(f"   HTTP ìƒíƒœ: {result['http_status']}")
        if 'exception_type' in result:
            print(f"   ì˜ˆì™¸ íƒ€ì…: {result['exception_type']}")
        return
    
    # ì„±ê³µí•œ ê²½ìš°
    game_info = result.get('data', {})
    print(f"=== {game_info.get('title', 'Unknown')} (ID: {game_info.get('app_id')}) ===")
    print(f"ğŸ“ ì§§ì€ ì„¤ëª…: {game_info.get('description', 'N/A')[:100]}...")
    
    detailed_desc = game_info.get('detailed_description', '')
    if detailed_desc:
        print(f"ğŸ“„ ìƒì„¸ ì„¤ëª…: {detailed_desc[:200]}...")
        print(f"   (ì´ {len(detailed_desc)} ë¬¸ì)")
    else:
        print(f"ğŸ“„ ìƒì„¸ ì„¤ëª…: N/A")
    
    print(f"ğŸ’° ê°€ê²©: {game_info.get('price_info', {}).get('current_price', 'N/A')}")
    print(f"ğŸ‘¨â€ğŸ’» ê°œë°œì‚¬: {game_info.get('developer_publisher', {}).get('developer', 'N/A')}")
    print(f"ğŸ¢ í¼ë¸”ë¦¬ì…”: {game_info.get('developer_publisher', {}).get('publisher', 'N/A')}")
    print(f"ğŸ“… ì¶œì‹œì¼: {game_info.get('release_date', 'N/A')}")
    print(f"â­ ì „ì²´ ë¦¬ë·°: {game_info.get('review_info', {}).get('all_reviews', 'N/A')}")
    print(f"ğŸ® ì¥ë¥´: {', '.join(game_info.get('genres', [])[:5]) or 'N/A'}")
    print(f"ğŸ·ï¸ íƒœê·¸: {', '.join(game_info.get('tags', [])[:5])}...")
    print(f"ğŸ–¼ï¸ í—¤ë” ì´ë¯¸ì§€: {game_info.get('header_images', ['N/A'])[0]}")


async def main(save=False):
    # ë¡œê±° ì„¤ì •
    setup_logger("INFO")
    
    # í…ŒìŠ¤íŠ¸
    test_games = [
        (1091500, "Cyberpunk 2077"),
        (2456740, "inZOI"),
        (570, "Dota 2"),
        (730, "Counter-Strike 2"),
        (203770,"CK2"),
        (1771300,"KCD2")
    ]
    
    all_results = []
    
    for app_id, expected_title in test_games:
        print(f"\n{'='*50}")
        print(f"í…ŒìŠ¤íŠ¸ ì¤‘: {expected_title} (ID: {app_id})")
        print(f"{'='*50}")
        
        result = await get_steam_game_info_crawler(app_id)
        print_game_info(result)
        
        if result.get('success', False) and save:
            # ê°œë³„ ê²Œì„ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥
            save_game_info_json(result)
        
        all_results.append(result)
    
    # ëª¨ë“  ê²Œì„ ì •ë³´ë¥¼ CSVë¡œ ì €ì¥
    if all_results and save:
        successful_count = len([r for r in all_results if r.get('success', False)])
        if successful_count > 0:
            save_multiple_games_csv(all_results, "test_games.csv")
            print(f"\nğŸ“Š ì´ {successful_count}ê°œ ê²Œì„ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ!")
            print("ğŸ’¾ ê°œë³„ JSON íŒŒì¼ê³¼ í†µí•© CSV íŒŒì¼ì´ data/game_info/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ ì €ì¥í•  ì„±ê³µí•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    asyncio.run(main(save=False))